{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "905354f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ef05954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
    "  bos_token='[BOS]', eos_token='[EOS]', unk_token='[UNK]', pad_token='[PAD]', mask_token='[MASK]'\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
    "  pad_token_id=tokenizer.eos_token_id, low_cpu_mem_usage=True, device_map='auto', load_in_8bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45f500b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuchan/.local/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.config/conda/envs/textgen/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.config/conda/envs/textgen/lib/python3.9/site-packages/transformers/generation/utils.py:1452\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1444\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1445\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1446\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1447\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1448\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1449\u001b[0m     )\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1452\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1453\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1466\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences \u001b[38;5;241m>\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mnum_beams:\n",
      "File \u001b[0;32m~/.config/conda/envs/textgen/lib/python3.9/site-packages/transformers/generation/utils.py:2482\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;66;03m# pre-process distribution\u001b[39;00m\n\u001b[1;32m   2481\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m logits_processor(input_ids, next_token_logits)\n\u001b[0;32m-> 2482\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m \u001b[43mlogits_warper\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2484\u001b[0m \u001b[38;5;66;03m# Store scores, attentions and hidden_states when required\u001b[39;00m\n\u001b[1;32m   2485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate:\n",
      "File \u001b[0;32m~/.config/conda/envs/textgen/lib/python3.9/site-packages/transformers/generation/logits_process.py:92\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m         scores \u001b[38;5;241m=\u001b[39m processor(input_ids, scores, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m~/.config/conda/envs/textgen/lib/python3.9/site-packages/transformers/generation/logits_process.py:297\u001b[0m, in \u001b[0;36mTopKLogitsWarper.__call__\u001b[0;34m(self, input_ids, scores)\u001b[0m\n\u001b[1;32m    295\u001b[0m top_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k, scores\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Safety check\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# Remove all tokens with a probability less than the last token of the top-k\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m indices_to_remove \u001b[38;5;241m=\u001b[39m scores \u001b[38;5;241m<\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    298\u001b[0m scores \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mmasked_fill(indices_to_remove, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_value)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    prompt = tokenizer.encode(\"한화의 김성근 감독님\", return_tensors='pt').to(device='cuda', non_blocking=True)\n",
    "    out = model.generate(**prompt, min_length=128, max_length=128, do_sample=True)\n",
    "    generated = tokenizer.batch_decode(out)[0]\n",
    "    \n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb9baa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc79eacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.cuda.amp import custom_fwd, custom_bwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df7017bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(bitsandbytes.nn.Linear8bitLt):\n",
    "    pass\n",
    "\n",
    "class LoRAEmbedding(bitsandbytes.nn.Linear8bitLt):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0839b20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def add_adapters(model, adapter_dim=16):\n",
    "    assert adapter_dim > 0\n",
    "\n",
    "    for module in model.modules():\n",
    "        if type(module) == bitsandbytes.nn.Linear8bitLt or type(module) == nn.Linear:\n",
    "            module.adapter = nn.Sequential(\n",
    "                LoRALinear(module.in_features, adapter_dim, bias=False),\n",
    "                LoRALinear(adapter_dim, module.out_features, bias=False),\n",
    "            )\n",
    "            nn.init.zeros_(module.adapter[1].weight)\n",
    "        elif type(module) == nn.Embedding:\n",
    "            module.adapter = nn.Sequential(\n",
    "                LoRAEmbedding(module.num_embeddings, adapter_dim),\n",
    "                LoRALinear(adapter_dim, module.embedding_dim, bias=False),\n",
    "            )\n",
    "            nn.init.zeros_(module.adapter[1].weight)\n",
    "\n",
    "add_adapters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca4c1d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTJForCausalLM(\n",
      "  (transformer): GPTJModel(\n",
      "    (wte): Embedding(\n",
      "      64512, 4096\n",
      "      (adapter): Sequential(\n",
      "        (0): LoRAEmbedding(in_features=64512, out_features=16, bias=True)\n",
      "        (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (12): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (13): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (14): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (15): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (16): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (17): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (18): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (19): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (20): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (21): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (22): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (23): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (24): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (25): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (26): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (27): GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (v_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (q_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear8bitLt(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear8bitLt(\n",
      "            in_features=4096, out_features=16384, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=16384, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (fc_out): Linear8bitLt(\n",
      "            in_features=16384, out_features=4096, bias=True\n",
      "            (adapter): Sequential(\n",
      "              (0): LoRALinear(in_features=16384, out_features=16, bias=False)\n",
      "              (1): LoRALinear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(\n",
      "    in_features=4096, out_features=64512, bias=True\n",
      "    (adapter): Sequential(\n",
      "      (0): LoRALinear(in_features=4096, out_features=16, bias=False)\n",
      "      (1): LoRALinear(in_features=16, out_features=64512, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a666eb73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
